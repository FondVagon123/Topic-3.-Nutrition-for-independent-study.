1. Призначення кластерного аналізу
Кластерний аналіз — це метод дослідження даних, який дозволяє групувати об'єкти у кластери (групи) так, щоб об'єкти всередині однієї групи були схожі між собою, а об'єкти з різних груп — відрізнялися.
Приклади застосування:
Маркетинг: сегментація клієнтів за поведінкою та витратами.


Біологія: класифікація видів рослин або тварин.


Соціологія: групування людей за соціальними характеристиками.


Приклад на Python:
from sklearn.cluster import KMeans
import numpy as np

# Приклад даних: витрати клієнтів на продукти А і В
X = np.array([[5, 2], [6, 1], [1, 6], [2, 7], [8, 1], [7, 2]])

kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)

print("Центри кластерів:", kmeans.cluster_centers_)
print("Приналежність до кластера:", kmeans.labels_)

Результат 
PS E:\Проекти пайтон> python tema3.py
Центри кластерів: [[1.5 6.5]
[6.5 1.5]]
Приналежність до кластера: [1 1 0 0 1 1]
PS E:\Проекти пайтон>


2. Відмінність кластерного аналізу від процесу класифікації
Кластерний аналіз:


Не потребує заздалегідь визначених міток (незалежний аналіз).


Мета — виявлення природних груп у даних.


Класифікація:


Виконується на основі наявних міток (під наглядом).


Мета — передбачити категорію нового об'єкта.


Приклад класифікації на Python:
from sklearn.tree import DecisionTreeClassifier

X = [[0, 0], [1, 1], [0, 1], [1, 0]]
y = [0, 1, 1, 0]

clf = DecisionTreeClassifier()
clf.fit(X, y)

print("Передбачення для нового об'єкта [0,1]:", clf.predict([[0,1]]))

Результат
PS E:\Проекти пайтон> python tema3.2.py
Передбачення для нового об'єкта [0,1]: [1]
PS E:\Проекти пайтон>


3. Особливість методу k-means
Метод k-means:
Групує об'єкти у k кластерів.
Кожен об'єкт належить до кластера з найближчим центром (середнім).
Центри кластерів перераховуються до стабілізації кластерів.
Приклад: сегментація клієнтів за віком і доходом.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Дані: [Вік, Дохід] для 5 клієнтів
X = np.array([[25, 40000], [30, 42000], [22, 38000], [35, 60000], [40, 62000]])

# Ініціалізація та навчання моделі k-means на 2 кластери
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)

# Отримання міток кластерів для кожного об'єкта
labels = kmeans.labels_

# Візуалізація результатів
plt.scatter(X[:,0], X[:,1], c=labels, cmap='viridis')
plt.xlabel("Вік")
plt.ylabel("Дохід")
plt.title("Кластеризація клієнтів методом k-means")
plt.show()
Результат



4. Для чого використовуються SOM-карти (Self-Organizing Maps)
SOM-карти — це нейронні мережі для зменшення розмірності даних та їх візуалізації.
Використовуються для:
Виявлення схожих груп у великих багатовимірних даних.
Візуалізації структури даних у 2D-просторі.


Приклад на Python (MiniSom):

Результат
Відповідні координати для об'єктів на карті:
(np.int64(2), np.int64(1))
(np.int64(1), np.int64(2))
(np.int64(0), np.int64(1))
(np.int64(2), np.int64(0))
(np.int64(1), np.int64(1))
(np.int64(1), np.int64(1))
(np.int64(2), np.int64(2))
(np.int64(0), np.int64(0))
(np.int64(2), np.int64(1))
(np.int64(0), np.int64(2))


5. Як навчати карти Кохонена (SOM)?
Карти Кохонена (Self-Organizing Maps, SOM) — це нейронні мережі для зменшення розмірності та кластеризації даних.
Навчання проходить у 3 етапи:


Ініціалізація ваг нейронів випадковими числами.
Для кожного вхідного вектору знаходиться найближчий нейрон (Best Matching Unit, BMU).
Ваги BMU та сусідніх нейронів оновлюються: W(t+1)=W(t)+η(t)⋅(X−W(t))
де η(t) — швидкість навчання, X — вхідний вектор.

Приклад на Python з MiniSom:
from minisom import MiniSom
import numpy as np

# Створення випадкових даних: 10 об'єктів, 4 ознаки
data = np.random.rand(10, 4)

# Ініціалізація SOM-карти (3x3 нейрони, 4 входи, параметри навчання)
som = MiniSom(3, 3, 4, sigma=0.5, learning_rate=0.5)

# Ініціалізація ваг випадковими значеннями на основі даних
som.random_weights_init(data)

# Навчання карти Кохонена на 100 ітерацій
som.train_random(data, 100)

# Виведення результатів
for d in data:
    print("Об'єкт:", d, "BMU:", som.winner(d))
Результат:



6. Що таке м’які обчислення?
М’які обчислення (Soft Computing) — методи, що дозволяють працювати з нечіткою, неповною або невизначеною інформацією.
Включають:


Нечіткі логічні системи (Fuzzy Logic)
Генетичні алгоритми
Нейронні мережі
Моделі на основі еволюційних обчислень
Приклад нечіткої логіки на Python:
import skfuzzy as fuzz
import numpy as np

# Створення універсальної множини для температури від 0 до 40 з кроком 1
x_temp = np.arange(0, 41, 1) 

# Визначення трикутних функцій приналежності для "низька" (low), "середня" (medium) та "висока" (high)
low = fuzz.trimf(x_temp, [0, 0, 20])
medium = fuzz.trimf(x_temp, [10, 20, 30])
high = fuzz.trimf(x_temp, [20, 40, 40])

print("Нечіткі значення температури:", low, medium, high)
Результати
Нечіткі значення температури: [1.  0.95 0.9  0.85 0.8  0.75 0.7  0.65 0.6  0.55 0.5  0.45 0.4  0.35 
0.3  0.25 0.2  0.15 0.1  0.05 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  
0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ] [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  
0.  0.  0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.  0.9 0.8 0.7 0.6 0.5 
0.4 0.3 0.2 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ] [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  
0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.05 0.1 
0.15 0.2 0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 
0.9  0.95 1.  ]


7. Класифікація видів м’яких обчислень
Види м’яких обчислень:
Вид
Призначення
Приклад
Нечіткі логічні системи (Fuzzy Logic)
Робота з нечіткими або неповними даними
Fuzzy-контролер кондиціонера
Нейронні мережі
Виявлення закономірностей, прогнозування
Розпізнавання рукописних цифр
Генетичні алгоритми
Оптимізація складних задач
Пошук оптимального маршруту доставки
Еволюційні обчислення
Пошук оптимальних рішень у складних просторах
Автоматичне проектування систем
Карти Кохонена (SOM)
Візуалізація і кластеризація даних
Сегментація клієнтів







Приклад Python (нечіткі множини):
import numpy as np
import skfuzzy as fuzz
import matplotlib.pyplot as plt

# Створення універсальної множини для температури від 0 до 40 з кроком 1
x_temp = np.arange(0, 41, 1)

# Визначення нечітких множин (трикутних функцій приналежності)
low = fuzz.trimf(x_temp, [0, 0, 20])
medium = fuzz.trimf(x_temp, [10, 20, 30])
high = fuzz.trimf(x_temp, [20, 40, 40])

# Побудова графіків
plt.plot(x_temp, low, label='Low')
plt.plot(x_temp, medium, label='Medium')
plt.plot(x_temp, high, label='High')
plt.xlabel("Температура")
plt.ylabel("Нечітка приналежність")
plt.title("Нечіткі множини")
plt.legend()
plt.show()

Результат



8. Ідея мурашиного алгоритму
Ant Colony Optimization (ACO) імітує поведінку мурашок при пошуку їжі.
Кожна мурашка залишає феромони на пройденому шляху.
Ймовірність вибору маршруту для наступних мурашок залежить від концентрації феромонів та довжини шляху.
З часом коротші шляхи отримують більше феромонів → алгоритм знаходить оптимальний маршрут.
Простий приклад на Python (симуляція короткого шляху):
import numpy as np

# Матриця відстаней (симуляція міст для алгоритму)
distances = np.array([
    [0, 2, 9, 10],
    [1, 0, 6, 4],
    [15, 7, 0, 8],
    [6, 3, 12, 0]
])

# Ініціалізація матриці феромонів (початкові значення 1.0)
pheromone = np.ones_like(distances, dtype=float)

# Простий цикл для симуляції розрахунку ймовірності
for step in range(4):
    # Розрахунок ймовірності: феромон / сума феромонів у рядку
    prob = pheromone / pheromone.sum(axis=1, keepdims=True)
    print(f"Ймовірності вибору шляху на кроці {step+1}:\n", prob)
Результат


9. Застосування мурашиного алгоритму для задачі Комівояжера
Мета: знайти найкоротший маршрут через усі міста.
Кожна мурашка будує маршрут, вибираючи наступне місто на основі:
Дистанції (чим ближче, тим краще)
Феромону (чим більше феромону, тим привабливіше)
Після обходу всіх маршрутів оновлюються феромони → через кілька ітерацій знаходиться оптимальний або близький до оптимального маршрут.
Python приклад :
import numpy as np
import random

# Матриця відстаней між містами (4 міста)
distances = np.array([
    [0, 2, 9, 10],
    [1, 0, 6, 4],
    [15, 7, 0, 8],
    [6, 3, 12, 0]
])

# Параметри алгоритму
NUM_ANTS = 5                 # Кількість мурашок
NUM_ITERATIONS = 10          # Кількість ітерацій (поколінь)
Pheromone = np.ones_like(distances, dtype=float)  # Ініціалізація феромону
alpha, beta = 1, 2           # Коефіцієнти впливу феромону (alpha) та евристики (beta)

# Основний цикл ітерацій
for iteration in range(NUM_ITERATIONS):
    paths = []
    # Цикл для кожної мурашки
    for ant in range(NUM_ANTS):
        visited = [0]
        # Побудова маршруту
        while len(visited) < distances.shape[0]:
            current = visited[-1]
            probabilities = []
            
            # Розрахунок ймовірностей переходу
            for next_city in range(distances.shape[0]):
                if next_city in visited:
                    probabilities.append(0)
                else:
                    # Формула ймовірності: (феромон^alpha) * (1/відстань^beta)
                    probabilities.append((Pheromone[current][next_city] ** alpha) * ((1 / distances[current][next_city]) ** beta))
            
            probabilities = np.array(probabilities)
            # Нормалізація ймовірностей
            p_probabilities = probabilities / probabilities.sum()
            
            # Вибір наступного міста на основі ймовірностей
            next_city = np.random.choice(range(distances.shape[0]), p=p_probabilities)
            visited.append(next_city)
        
        # Завершення циклу (повернення в початкове місто)
        visited.append(visited[0])
        paths.append(visited)

    # Випаровування феромону (глобальне)
    Pheromone *= 0.9  # Коефіцієнт випаровування (rho=0.1, залишається 1-rho=0.9)

    # Оновлення феромону (нанесення на кращі шляхи)
    for path in paths:
        # Обчислення довжини маршруту
        length = sum(distances[path[i]][path[i+1]] for i in range(len(path)-1))
        # Нанесення феромону: 1/довжина
        for i in range(len(path)-1):
            Pheromone[path[i]][path[i+1]] += 1 / length

print("Фінальні феромони:\n", Pheromone)
Результат
Фінальні феромони:
[[0.34867844 2.10263474 0.42320294 0.34867844]
[0.34867844 0.34867844 0.59649115 1.92934653]
[0.34867844 0.42320294 0.34867844 0.59649115]
[0.34867844 0.34867844 1.85482203 0.34867844]]
PS E:\Проекти пайтон>



